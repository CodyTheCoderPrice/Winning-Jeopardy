{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1715f7e",
   "metadata": {},
   "source": [
    "# Winning Jeopardy: Finding the Leading Strategies\n",
    "[Jeopardy](https://en.wikipedia.org/wiki/Jeopardy!) is an iconic TV quiz show in the USA that reverses the typical questiong/answer format of most other quiz shows. The contestants are instead presented with general knowledge clues in the form of answers and must phrase their responses in the form of questions to earn money.\n",
    "\n",
    "The goal of this project is to find patterns in the questions so we can find a winning strategy. We'll work with a dataset containing over 200,000 rows from the beginning of a full dataset of Jeopardy questions, available for downloading [here](https://www.reddit.com/r/datasets/comments/1uyd0t/200000_jeopardy_questions_in_a_json_file). Each row represents a single question on a single episode of Jeopardy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "712792d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 216930\n",
      "Number of columns: 7\n",
      "Number of missing values: 3637\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show Number</th>\n",
       "      <th>Air Date</th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>Copernicus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>$200</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>McDonald's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>$200</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>John Adams</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Show Number    Air Date      Round                         Category  Value  \\\n",
       "0         4680  2004-12-31  Jeopardy!                          HISTORY   $200   \n",
       "1         4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES   $200   \n",
       "2         4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...   $200   \n",
       "3         4680  2004-12-31  Jeopardy!                 THE COMPANY LINE   $200   \n",
       "4         4680  2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES   $200   \n",
       "\n",
       "                                            Question      Answer  \n",
       "0  For the last 8 years of his life, Galileo was ...  Copernicus  \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe  \n",
       "2  The city of Yuma in this state has a record av...     Arizona  \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's  \n",
       "4  Signer of the Dec. of Indep., framer of the Co...  John Adams  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "\n",
    "jeopardy = pd.read_csv(\"jeopardy.csv\")\n",
    "\n",
    "print(f'Number of rows: {jeopardy.shape[0]}')\n",
    "print(f'Number of columns: {jeopardy.shape[1]}')\n",
    "print(f'Number of missing values: {jeopardy.isnull().sum().sum()}')\n",
    "\n",
    "jeopardy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e55c08e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Show Number', ' Air Date', ' Round', ' Category', ' Value',\n",
       "       ' Question', ' Answer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eee28de",
   "metadata": {},
   "source": [
    "- `Show Number` - the Jeopardy episode number\n",
    "- `Air Date` - the date the episode aired\n",
    "- `Round` - the round of Jeopardy\n",
    "- `Category` - the category of the question\n",
    "- `Value` - the number of dollars the correct answer is worth\n",
    "- `Question` - the text of the question\n",
    "- `Answer` - the text of the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2179ac27",
   "metadata": {},
   "source": [
    "## Cleaning the Data\n",
    "Our dataset has 3637 missing values and most of our column names have unwanted spaces in front of them. We'll need to clean our data before we begin our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48515463",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Show Number', 'Air Date', 'Round', 'Category', 'Value', 'Question',\n",
       "       'Answer'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing leading spaces from column names\n",
    "cleaned_column_names = []\n",
    "for column in jeopardy.columns:\n",
    "    cleaned_column_names.append(column.lstrip())\n",
    "\n",
    "jeopardy.columns = cleaned_column_names\n",
    "jeopardy.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b8c1b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Removing rows with missing values\n",
    "jeopardy = jeopardy.dropna()\n",
    "print(f'Number of missing values: {jeopardy.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d8a68e",
   "metadata": {},
   "source": [
    "## Normalizing Columns\n",
    "To make our data easier to work with, we'll normalize our the data in the following columns:\n",
    "- `Question` and `Answer` – making text lowercase and removing punctuation.\n",
    "- `Value` – removing the dollar signs and converting each value to numeric.\n",
    "- `Air Date` – convert to datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e21dd08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Show Number</th>\n",
       "      <th>Air Date</th>\n",
       "      <th>Round</th>\n",
       "      <th>Category</th>\n",
       "      <th>Value</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>clean_question</th>\n",
       "      <th>clean_answer</th>\n",
       "      <th>clean_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>HISTORY</td>\n",
       "      <td>$200</td>\n",
       "      <td>For the last 8 years of his life, Galileo was ...</td>\n",
       "      <td>Copernicus</td>\n",
       "      <td>for the last 8 years of his life galileo was u...</td>\n",
       "      <td>copernicus</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>ESPN's TOP 10 ALL-TIME ATHLETES</td>\n",
       "      <td>$200</td>\n",
       "      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n",
       "      <td>Jim Thorpe</td>\n",
       "      <td>no 2 1912 olympian football star at carlisle i...</td>\n",
       "      <td>jim thorpe</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EVERYBODY TALKS ABOUT IT...</td>\n",
       "      <td>$200</td>\n",
       "      <td>The city of Yuma in this state has a record av...</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>the city of yuma in this state has a record av...</td>\n",
       "      <td>arizona</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>THE COMPANY LINE</td>\n",
       "      <td>$200</td>\n",
       "      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>in 1963 live on the art linkletter show this c...</td>\n",
       "      <td>mcdonalds</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4680</td>\n",
       "      <td>2004-12-31</td>\n",
       "      <td>Jeopardy!</td>\n",
       "      <td>EPITAPHS &amp; TRIBUTES</td>\n",
       "      <td>$200</td>\n",
       "      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n",
       "      <td>John Adams</td>\n",
       "      <td>signer of the dec of indep framer of the const...</td>\n",
       "      <td>john adams</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Show Number   Air Date      Round                         Category Value  \\\n",
       "0         4680 2004-12-31  Jeopardy!                          HISTORY  $200   \n",
       "1         4680 2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES  $200   \n",
       "2         4680 2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...  $200   \n",
       "3         4680 2004-12-31  Jeopardy!                 THE COMPANY LINE  $200   \n",
       "4         4680 2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES  $200   \n",
       "\n",
       "                                            Question      Answer  \\\n",
       "0  For the last 8 years of his life, Galileo was ...  Copernicus   \n",
       "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe   \n",
       "2  The city of Yuma in this state has a record av...     Arizona   \n",
       "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's   \n",
       "4  Signer of the Dec. of Indep., framer of the Co...  John Adams   \n",
       "\n",
       "                                      clean_question clean_answer  clean_value  \n",
       "0  for the last 8 years of his life galileo was u...   copernicus          200  \n",
       "1  no 2 1912 olympian football star at carlisle i...   jim thorpe          200  \n",
       "2  the city of yuma in this state has a record av...      arizona          200  \n",
       "3  in 1963 live on the art linkletter show this c...    mcdonalds          200  \n",
       "4  signer of the dec of indep framer of the const...   john adams          200  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^A-Za-z0-9\\s]\", \"\", text)\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def normalize_value(text):\n",
    "    text = re.sub(\"[^A-Za-z0-9\\s]\", \"\", text)\n",
    "    try:\n",
    "        text = int(text)\n",
    "    except Exception:\n",
    "        text = 0\n",
    "    return text\n",
    "\n",
    "jeopardy['clean_question'] = jeopardy['Question'].apply(normalize_text)\n",
    "jeopardy['clean_answer'] = jeopardy['Answer'].apply(normalize_text)\n",
    "jeopardy['clean_value'] = jeopardy['Value'].apply(normalize_value)\n",
    "jeopardy['Air Date'] = pd.to_datetime(jeopardy['Air Date'])\n",
    "\n",
    "jeopardy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47af7e11",
   "metadata": {},
   "source": [
    "## Answers in Questions\n",
    "In order to figure out whether to study past questions, study general knowledge, or not study it all, it would be helpful to figure out two things:\n",
    "- How often the answer can be used for a question.\n",
    "- How often questions are repeated.\n",
    "\n",
    "To answer the first question, we can check how many times words in the answer also occur in the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4baa5434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.058228398435179705"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_matches(row):\n",
    "    split_answer = row[\"clean_answer\"].split()\n",
    "    split_question = row[\"clean_question\"].split()\n",
    "    if \"the\" in split_answer:\n",
    "        split_answer.remove(\"the\")\n",
    "    if len(split_answer) == 0:\n",
    "        return 0\n",
    "    match_count = 0\n",
    "    for item in split_answer:\n",
    "        if item in split_question:\n",
    "            match_count += 1\n",
    "    return match_count / len(split_answer)\n",
    "\n",
    "jeopardy[\"answer_in_question\"] = jeopardy.apply(count_matches, axis=1)\n",
    "\n",
    "jeopardy[\"answer_in_question\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c122f",
   "metadata": {},
   "source": [
    "The answer is mentioned in the question roughly 6% of the time. This is not often enough to be a winning strategy, so let's figure out the answer to our second question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e345372",
   "metadata": {},
   "source": [
    "## Recycled Questions\n",
    "Now we're going to find out how often questions are repeated.\n",
    "\n",
    "The idea is to check if the words in the questions have been used before. To ensure the words are meaningful, we'll exclude common or filler words such as *the* and *it*. To filter out insignificant words, we'll use a stop word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5389416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['each', 'others', 'whereafter', 'several', 'being', 'and', \"she'll\", 'was', 'they', 'see', 'nobody', 'show', 'before', 'whose', \"we've\", 'became', 'thereafter', 'nor', 'who', 'thereby', 'the', 'down', \"they'd\", 'how', 'were', 'ltd', 'otherwise', 'your', 'when', 'do', 'behind', 'anyone', 'their', 'detail', \"it's\", \"they'll\", 'already', 'few', 'further', 'some', 'both', 'beforehand', 'less', 'give', 'twelve', 'them', 'made', 'since', 'empty', 'eleven', '1', 'while', 'rather', 'becoming', 'myself', 'it', \"i'd\", 'regarding', 'except', 're', 'describe', 'above', 'are', \"wasn't\", 'couldnt', 'against', '3', 'sincere', \"we'll\", 'whereupon', 'wherein', 'never', 'someone', 'nevertheless', 'keep', 'out', 'without', 'doesn', 'below', 'us', 'third', 'sixty', 'fill', 'ought', 'five', 'beyond', 'full', 'take', 'say', 'sometime', 'this', 'get', 'because', 'un', 'upon', 'indeed', \"why's\", 'thus', '5', 'thick', \"he'll\", 'sometimes', \"that's\", '4', 'is', 'latterly', 'enough', 'namely', 'eight', 'hereupon', 'about', 'really', 'our', 'these', 'thin', 'across', 'next', \"isn't\", 'most', \"hasn't\", \"haven't\", 'elsewhere', 'does', \"you're\", 'after', 'herself', 'may', 'she', 'hundred', 'mine', \"doesn't\", 'along', 'hereby', 'within', 'three', \"she's\", 'did', 'forty', \"you'd\", 'to', 'seeming', 'nowhere', 'mostly', 'whether', 'him', 'such', 'quite', 'of', 'latter', 'done', 'become', 'seems', 'than', 'various', \"i'm\", 'through', 'many', \"who's\", 'noone', 'doing', \"here's\", \"what's\", 'there', 'its', 'themselves', 'me', 'a', \"weren't\", 'anyhow', 'per', 'found', 'fifty', '8', \"mustn't\", 'whom', 'no', 'unless', 'first', \"don't\", 'much', '7', 'all', 'another', 'somewhere', 'throughout', 'neither', 'might', 'that', 'cannot', 'on', 'we', 'more', 'himself', 'hers', \"we're\", \"they're\", 'cant', 'twenty', \"i've\", 'between', 'hasnt', 'until', 'nine', 'nothing', 'computer', 'name', 'four', 'call', 'please', 'over', \"you've\", \"how's\", 'ours', 'front', 'have', 'not', 'he', 'used', '9', \"they've\", 'again', 'just', 'one', 'yourself', 'seemed', 'amoungst', 'yours', 'serious', 'would', 'least', 'however', 'theirs', 'mill', 'always', 'hereafter', 'using', '2', 'therein', 'why', 'what', 'alone', 'side', 'co', 'every', 'well', 'here', 'even', 'beside', 'only', \"shouldn't\", 'whoever', 'own', \"he'd\", 'which', 'during', 'toward', 'something', 'seem', 'whole', 'perhaps', \"can't\", 'km', 'becomes', 'having', 'or', \"you'll\", 'thence', 'top', 'so', 'none', 'same', 'has', 'up', 'still', 'be', 'wherever', 'anyway', 'also', 'herein', 'must', '0', 'yet', 'ie', 'meanwhile', 'everyone', 'you', 'now', 'go', 'interest', 'whatever', 'my', 'in', 'fire', \"he's\", 'don', 'moreover', 'among', \"wouldn't\", 'those', 'yourselves', 'though', 'with', 'last', \"aren't\", 'formerly', 'find', 'her', 'two', 'from', 'once', \"there's\", 'thereupon', 'his', 'kg', 'move', 'at', 'whither', 'afterwards', \"hadn't\", 'if', \"where's\", 'six', \"didn't\", 'onto', 'ourselves', 'whence', 'very', 'around', 'amount', 'any', 'system', 'i', \"she'd\", \"couldn't\", 'anything', '6', 'off', 'bill', 'therefore', 'fifteen', 'inc', 'almost', 'amongst', 'former', 'can', 'could', 'by', 'thru', 'make', \"shan't\", 'de', 'as', 'although', 'had', 'towards', 'due', 'via', 'will', 'whenever', \"we'd\", 'hence', 'where', 'somehow', 'too', 'everywhere', 'didn', 'everything', 'else', 'together', 'but', 'ten', \"let's\", 'eg', 'under', 'often', 'into', 'cry', 'etc', 'ever', 'been', 'besides', 'bottom', 'am', 'either', 'should', 'for', 'itself', 'then', 'part', 'anywhere', \"won't\", 'whereas', 'put', 'back', 'whereby', \"when's\", 'an', 'con', 'other', \"i'll\"]\n"
     ]
    }
   ],
   "source": [
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('en')\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# Combining stop word lists from different libraries \n",
    "stopword_list = list(set(stop_words+list(STOPWORDS)+list(ENGLISH_STOP_WORDS)+['1','2','3','4','5','6','7','8','9','0']))\n",
    "\n",
    "print(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e109a6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92% of meaningful word overlap in questions.\n"
     ]
    }
   ],
   "source": [
    "question_overlap = []\n",
    "terms_used = set()\n",
    "terms_used_list = []\n",
    "\n",
    "jeopardy.sort_values(by=['Air Date'])\n",
    "\n",
    "for i, row in jeopardy.iterrows():\n",
    "    split_question=row['clean_question'].split(' ')\n",
    "    match_count = 0\n",
    "    \n",
    "    # Removing stop words\n",
    "    split_question = [word for word in split_question if word not in stopword_list]\n",
    "    \n",
    "    for word in split_question:\n",
    "        if word in terms_used:\n",
    "            match_count += 1     # the number of repeated words\n",
    "            \n",
    "    for word in split_question:\n",
    "        terms_used.add(word)   # a set of unique words in all the questions\n",
    "        terms_used_list.append(word)\n",
    "        \n",
    "    if len(split_question) > 0:\n",
    "        match_count /= len(split_question)\n",
    "        \n",
    "    question_overlap.append(match_count)\n",
    "    \n",
    "jeopardy['question_overlap'] = question_overlap\n",
    "question_overlap_pct = round(jeopardy['question_overlap'].mean()*100)\n",
    "print(f'{question_overlap_pct}% of meaningful word overlap in questions.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e607c31",
   "metadata": {},
   "source": [
    "There is about a 92% overlap between terms in new questions and terms in old questions. This is significant enough to be worthy of looking into the recycling of questions more in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9df99d9",
   "metadata": {},
   "source": [
    "## Low Value vs High Value Questions\n",
    "Since we want to earn as much money as we can on Jeopardy, let's figure out which words correspond to high-value questions using a chi-squared test, so we can know which topics are most worth studying. First, we'll need to split the questions into two categories:\n",
    "- low value – less than or equal to \\\\$800.\n",
    "- high value – greater than \\\\$800.\n",
    "\n",
    "We're going to perform the chi-squared test on a hand full of the most frequent words in our dataset to see which ones correlate the most with high value questions. We're limiting the number of words because using the chi-squared test across all the words in our dataset would take a very long time to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1afa12e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_value(row):\n",
    "    if row['clean_value'] > 800:\n",
    "        value = 1\n",
    "    else:\n",
    "        value = 0\n",
    "    return value\n",
    "\n",
    "jeopardy['high_value'] = jeopardy.apply(determine_value, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accbd019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 25 most frequent words in the whole dataset:\n",
      "['city', 'named', 'called', 'like', 'seen', 'new', 'country', 'man', 'type', 'film', 'clue', 'state', 'title', 'crew', 'known', 'word', 'said', 'years', 'played', 'novel', 'term', 'wrote', 'president', 'american', 'capital']\n"
     ]
    }
   ],
   "source": [
    "# Creating a list of the 25 most frequent words in all the questions\n",
    "comparison_terms = list(pd.Series(terms_used_list).value_counts()[:25].index)\n",
    "\n",
    "print(f'The 25 most frequent words in the whole dataset:'\n",
    "      f'\\n{comparison_terms}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073f630c",
   "metadata": {},
   "source": [
    "Many of these words are not useful topics of study, such as *like* or *seen*, so we will be removing them so we're only left with topics people can prepare for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5db1436f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Useful comparison terms for studying:\n",
      "['city', 'country', 'film', 'state', 'title', 'years', 'novel', 'wrote', 'president', 'american', 'capital']\n"
     ]
    }
   ],
   "source": [
    "words_to_remove = ['named', 'called', 'like', 'seen', 'new', 'man', 'type', 'clue', 'crew', 'known', 'word', 'said', 'played', 'term']\n",
    "comparison_terms = [x for x in comparison_terms if x not in words_to_remove]\n",
    "\n",
    "print(f'Useful comparison terms for studying:'\n",
    "      f'\\n{comparison_terms}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc2aa44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of times each word occurred in high and low value questions:\n",
      "[(1830, 4430), (1375, 3318), (1361, 3197), (1152, 3195), (1345, 2603), (873, 2110), (1022, 1849), (964, 1847), (814, 1955), (913, 1817), (772, 1853)]\n"
     ]
    }
   ],
   "source": [
    "def count_usage(word):\n",
    "    low_count = 0\n",
    "    high_count = 0\n",
    "    for i, row in jeopardy.iterrows():\n",
    "        split_question=row['clean_question'].split(' ')\n",
    "        if word in split_question:\n",
    "            if row['high_value'] == 1:\n",
    "                high_count += 1\n",
    "            else:\n",
    "                low_count += 1\n",
    "    return high_count, low_count\n",
    "\n",
    "# Extracting high and low values for each of the comparison terms\n",
    "observed_high_low = []\n",
    "for word in comparison_terms:\n",
    "    observed_high_low.append(count_usage(word))\n",
    "    \n",
    "print(f'\\nNumber of times each word occurred in high and low value questions:'\n",
    "      f'\\n{observed_high_low}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e72b8ef",
   "metadata": {},
   "source": [
    "## Applying the Chi-Squared Test\n",
    "Now that we've found the observed counts for our comparison terms, we can compute the expected counts, chi-squared values, and p-values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8145a86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Power_divergenceResult(statistic=0.5809512438929748, pvalue=0.4459397050776779),\n",
       " Power_divergenceResult(statistic=0.5766656287591106, pvalue=0.4476222639375934),\n",
       " Power_divergenceResult(statistic=2.5098772876099753, pvalue=0.11313474098463278),\n",
       " Power_divergenceResult(statistic=11.1757612387094, pvalue=0.0008287288635621066),\n",
       " Power_divergenceResult(statistic=53.493013668195644, pvalue=2.5951408355367423e-13),\n",
       " Power_divergenceResult(statistic=0.3197743857612405, pvalue=0.5717432623900001),\n",
       " Power_divergenceResult(statistic=64.75146512443725, pvalue=8.496641676371857e-16),\n",
       " Power_divergenceResult(statistic=41.42296927532989, pvalue=1.2260824877894515e-10),\n",
       " Power_divergenceResult(statistic=0.4859780120408699, pvalue=0.4857269278885131),\n",
       " Power_divergenceResult(statistic=28.741930404210784, pvalue=8.269376867432918e-08),\n",
       " Power_divergenceResult(statistic=0.48030800377642013, pvalue=0.4882828366922436)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import chisquare\n",
    "\n",
    "# Counting high and low value questions\n",
    "high_value_count = len(jeopardy[jeopardy['high_value'] == 1])\n",
    "low_value_count = len(jeopardy[jeopardy['high_value'] == 0])\n",
    "\n",
    "# Counting chi-squared and p-values for each word\n",
    "chi_squared = []\n",
    "for item in observed_high_low:\n",
    "    total = item[0] + item[1] # the number of questions a word occurs in\n",
    "    total_prop = total / len(jeopardy)\n",
    "    expected_high = total_prop * high_value_count\n",
    "    expected_low = total_prop * low_value_count\n",
    "    \n",
    "    observed = np.array([item[0], item[1]])\n",
    "    expected = np.array([expected_high, expected_low])\n",
    "    chi_squared.append(chisquare(observed, expected))\n",
    "\n",
    "chi_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9ea514",
   "metadata": {},
   "source": [
    "To make these results more readable, let's create a dataframe and only include the words with p-value lower than 0.05, meaning that the results are significant and cannot be explained just by a random chance. We'll also include how often each word appears in high and low value questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d1e978e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Chi_squared</th>\n",
       "      <th>p_val</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>novel</td>\n",
       "      <td>64.751465</td>\n",
       "      <td>8.496642e-16</td>\n",
       "      <td>1022</td>\n",
       "      <td>1849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>title</td>\n",
       "      <td>53.493014</td>\n",
       "      <td>2.595141e-13</td>\n",
       "      <td>1345</td>\n",
       "      <td>2603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wrote</td>\n",
       "      <td>41.422969</td>\n",
       "      <td>1.226082e-10</td>\n",
       "      <td>964</td>\n",
       "      <td>1847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>american</td>\n",
       "      <td>28.741930</td>\n",
       "      <td>8.269377e-08</td>\n",
       "      <td>913</td>\n",
       "      <td>1817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>state</td>\n",
       "      <td>11.175761</td>\n",
       "      <td>8.287289e-04</td>\n",
       "      <td>1152</td>\n",
       "      <td>3195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Word  Chi_squared         p_val  High   Low\n",
       "0     novel    64.751465  8.496642e-16  1022  1849\n",
       "1     title    53.493014  2.595141e-13  1345  2603\n",
       "2     wrote    41.422969  1.226082e-10   964  1847\n",
       "3  american    28.741930  8.269377e-08   913  1817\n",
       "4     state    11.175761  8.287289e-04  1152  3195"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_squared_list = []\n",
    "for i in range(len(comparison_terms)):\n",
    "    chi_squared_row = []\n",
    "    \n",
    "    # Adding the word associated with each pair of chi-squared and p-value \n",
    "    chi_squared_row.append(comparison_terms[i])  \n",
    "    \n",
    "    chi_squared_row.append(list(chi_squared[i])[0]) # chi squared value\n",
    "    chi_squared_row.append(list(chi_squared[i])[1]) # p value\n",
    "    chi_squared_row.append(observed_high_low[i][0]) # number of appearances in high value questions\n",
    "    chi_squared_row.append(observed_high_low[i][1]) # number of appearances in low value questions\n",
    "    chi_squared_list.append(chi_squared_row)\n",
    "    \n",
    "readable_chi_squared = pd.DataFrame(chi_squared_list, columns=['Word', 'Chi_squared', 'p_val', 'High', 'Low'])\n",
    "readable_chi_squared = readable_chi_squared.sort_values(by=['Chi_squared'], ascending=False).reset_index(drop=True)\n",
    "readable_chi_squared = readable_chi_squared[readable_chi_squared['p_val']<0.05]\n",
    "\n",
    "readable_chi_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d811fa",
   "metadata": {},
   "source": [
    "While these 5 words have p-values that are statistically significant, meaning they are more likely to appear in either high or low value questions, the p-value does not tell us which one it is. To determine this, we included the how often each word appears in both types of questions in our dataframe. Unfortuneatly for us, all 5 of these words appear more often in low value questions, meaning this is what our p-value is indicating.\n",
    "\n",
    "Let's see if we can find an explanation for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccd133a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of high value questions: 61,422\n",
      "Number of low value questions:  151,871\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of high value questions: {high_value_count:,}'\n",
    "      f'\\nNumber of low value questions:  {low_value_count:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f32a5f4",
   "metadata": {},
   "source": [
    "It makes sense that given most questions are of low value, it makes sense that our Chi Squared test would find words that are more likely to appear in low value questions, rather than high value questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5154f331",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We succesfully found that the following five words appear commonly in jeopardy questions and are therefore topics worth studying:\n",
    "- Novel\n",
    "- Title\n",
    "- Wrote\n",
    "- American\n",
    "- State\n",
    "\n",
    "Unfortunately, we did not find any commonly used words that appear more often in high value questions (worth more than \\\\$800)  than low value questions (worth \\\\$800 or less), which would have further improved our odds of winning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
